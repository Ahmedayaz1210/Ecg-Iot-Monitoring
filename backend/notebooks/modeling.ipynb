{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "480b09db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC, F1Score, BinaryAccuracy\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a18596d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../datasets/filtered_ecg_signals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3ec6a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (14552, 187), y: (14552,)\n",
      "X Head:           0         1         2         3         4         5         6  \\\n",
      "0 -0.008763 -0.135806 -0.577641 -0.933554 -0.914556 -0.802527 -0.793689   \n",
      "1  0.063421 -0.155628 -0.509913 -0.825991 -0.878442 -0.729940 -0.603986   \n",
      "2  0.022680 -0.112090 -0.397416 -0.691927 -0.824502 -0.812865 -0.754373   \n",
      "3  0.019383 -0.517860 -0.852588 -0.900991 -0.833111 -0.827430 -0.843862   \n",
      "4  0.015131 -0.192058 -0.649444 -0.894625 -0.798463 -0.757614 -0.810288   \n",
      "\n",
      "          7         8         9  ...       177       178       179       180  \\\n",
      "0 -0.799524 -0.782339 -0.773429  ... -0.024296 -0.025436 -0.026514 -0.027526   \n",
      "1 -0.570665 -0.548560 -0.509061  ... -0.022942 -0.022344 -0.021756 -0.021177   \n",
      "2 -0.717723 -0.709217 -0.699311  ... -0.126305 -0.123111 -0.119005 -0.114365   \n",
      "3 -0.824888 -0.809077 -0.804276  ... -0.042515 -0.043941 -0.045268 -0.046491   \n",
      "4 -0.793197 -0.763684 -0.765459  ... -0.045252 -0.046334 -0.047327 -0.048227   \n",
      "\n",
      "        181       182       183       184       185       186  \n",
      "0 -0.028469 -0.029338 -0.030130 -0.030841 -0.031466 -0.032002  \n",
      "1 -0.020607 -0.020044 -0.019486 -0.018933 -0.018384 -0.017837  \n",
      "2 -0.109866 -0.104885 -0.099466 -0.093852 -0.087874 -0.081501  \n",
      "3 -0.047605 -0.048606 -0.049489 -0.050249 -0.050880 -0.051379  \n",
      "4 -0.049030 -0.049731 -0.050325 -0.050809 -0.051178 -0.051428  \n",
      "\n",
      "[5 rows x 187 columns]\n",
      "y Head: 0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: label, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14552 entries, 0 to 14551\n",
      "Columns: 187 entries, 0 to 186\n",
      "dtypes: float64(187)\n",
      "memory usage: 20.8 MB\n",
      "X Info: None\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 14552 entries, 0 to 14551\n",
      "Series name: label\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "14552 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 113.8 KB\n",
      "y Info: None\n",
      "X Description:                   0             1             2             3             4  \\\n",
      "count  14552.000000  14552.000000  14552.000000  14552.000000  14552.000000   \n",
      "mean      -0.000039     -0.257087     -0.534548     -0.705954     -0.724540   \n",
      "std        0.030436      0.192234      0.246273      0.246400      0.213137   \n",
      "min       -0.216357     -1.003281     -1.068609     -1.082750     -1.095827   \n",
      "25%       -0.018335     -0.390852     -0.722644     -0.888518     -0.863630   \n",
      "50%        0.000578     -0.247247     -0.567740     -0.774344     -0.792935   \n",
      "75%        0.016610     -0.109577     -0.378187     -0.609285     -0.667127   \n",
      "max        0.142053      0.416879      0.418889      0.200370      0.243403   \n",
      "\n",
      "                  5             6             7             8             9  \\\n",
      "count  14552.000000  14552.000000  14552.000000  14552.000000  14552.000000   \n",
      "mean      -0.697936     -0.686106     -0.671671     -0.654474     -0.641804   \n",
      "std        0.184988      0.172437      0.164630      0.160779      0.158620   \n",
      "min       -1.091257     -1.068060     -0.960776     -0.961244     -0.982793   \n",
      "25%       -0.815632     -0.802269     -0.785326     -0.768303     -0.756725   \n",
      "50%       -0.749320     -0.733142     -0.717112     -0.698797     -0.684273   \n",
      "75%       -0.647825     -0.627655     -0.601613     -0.579997     -0.560778   \n",
      "max        0.256671      0.305832      0.317625      0.334037      0.373186   \n",
      "\n",
      "       ...           177           178           179           180  \\\n",
      "count  ...  14552.000000  14552.000000  14552.000000  14552.000000   \n",
      "mean   ...     -0.021929     -0.022478     -0.022861     -0.023236   \n",
      "std    ...      0.037020      0.036682      0.035356      0.032951   \n",
      "min    ...     -0.324857     -0.310690     -0.295848     -0.279843   \n",
      "25%    ...     -0.032939     -0.033661     -0.034350     -0.034934   \n",
      "50%    ...     -0.020322     -0.021436     -0.022399     -0.023310   \n",
      "75%    ...     -0.005629     -0.006342     -0.006916     -0.007424   \n",
      "max    ...      0.558979      0.646638      0.724785      0.523144   \n",
      "\n",
      "                181           182           183           184           185  \\\n",
      "count  14552.000000  14552.000000  14552.000000  14552.000000  14552.000000   \n",
      "mean      -0.023566     -0.023792     -0.024052     -0.024267     -0.024327   \n",
      "std        0.031810      0.030783      0.028591      0.026970      0.025891   \n",
      "min       -0.263361     -0.245852     -0.227268     -0.207853     -0.187470   \n",
      "25%       -0.035561     -0.036199     -0.036726     -0.037229     -0.037680   \n",
      "50%       -0.024110     -0.024839     -0.025440     -0.026068     -0.026526   \n",
      "75%       -0.007917     -0.008333     -0.008734     -0.009191     -0.009646   \n",
      "max        0.497859      0.584549      0.439504      0.364404      0.307299   \n",
      "\n",
      "                186  \n",
      "count  14552.000000  \n",
      "mean      -0.024344  \n",
      "std        0.024460  \n",
      "min       -0.165981  \n",
      "25%       -0.037892  \n",
      "50%       -0.026953  \n",
      "75%       -0.010084  \n",
      "max        0.055172  \n",
      "\n",
      "[8 rows x 187 columns]\n",
      "y Description: count    14552.000000\n",
      "mean         0.721963\n",
      "std          0.448047\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          1.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: label, dtype: float64\n",
      "X tail:               0         1         2         3         4         5         6  \\\n",
      "14547  0.023918  0.011518 -0.324441 -0.656283 -0.716222 -0.681565 -0.667860   \n",
      "14548 -0.026852 -0.004463 -0.029894 -0.101966 -0.174720 -0.221850 -0.267967   \n",
      "14549 -0.034974 -0.154461 -0.328385 -0.518593 -0.677296 -0.794440 -0.855589   \n",
      "14550 -0.036629 -0.082905 -0.295897 -0.479491 -0.527493 -0.575216 -0.646335   \n",
      "14551  0.005644 -0.275933 -0.497106 -0.629898 -0.692630 -0.733745 -0.775428   \n",
      "\n",
      "              7         8         9  ...       177       178       179  \\\n",
      "14547 -0.636358 -0.608730 -0.598535  ... -0.017762 -0.018862 -0.019912   \n",
      "14548 -0.332249 -0.384950 -0.393488  ...  0.010762  0.009834  0.008906   \n",
      "14549 -0.859433 -0.832900 -0.804202  ... -0.081176 -0.081148 -0.081011   \n",
      "14550 -0.673452 -0.670596 -0.664000  ... -0.021255 -0.021589 -0.021897   \n",
      "14551 -0.795872 -0.789673 -0.780998  ... -0.051390 -0.052590 -0.053685   \n",
      "\n",
      "            180       181       182       183       184       185       186  \n",
      "14547 -0.020908 -0.021847 -0.022725 -0.023538 -0.024282 -0.024953 -0.025548  \n",
      "14548  0.007980  0.007061  0.006150  0.005251  0.004368  0.003503  0.002659  \n",
      "14549 -0.080786 -0.080457 -0.080016 -0.079469 -0.078809 -0.078030 -0.077133  \n",
      "14550 -0.022178 -0.022427 -0.022643 -0.022823 -0.022964 -0.023064 -0.023120  \n",
      "14551 -0.054672 -0.055546 -0.056302 -0.056936 -0.057443 -0.057818 -0.058057  \n",
      "\n",
      "[5 rows x 187 columns]\n",
      "y tail: 14547    1\n",
      "14548    1\n",
      "14549    1\n",
      "14550    1\n",
      "14551    1\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:, :-1] # features\n",
    "\n",
    "y = data.iloc[:, -1] # labels\n",
    "\n",
    "print(f\"X: {X.shape}, y: {y.shape}\")\n",
    "print(f\"X Head: {X.head()}\")\n",
    "print(f\"y Head: {y.head()}\")\n",
    "print(f\"X Info: {X.info()}\")\n",
    "print(f\"y Info: {y.info()}\")\n",
    "print(f\"X Description: {X.describe()}\")\n",
    "print(f\"y Description: {y.describe()}\")\n",
    "print(f\"X tail: {X.tail()}\")\n",
    "print(f\"y tail: {y.tail()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa4d4b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAGwCAYAAAC0HlECAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAmNklEQVR4nO3dcXDU9Z3/8deakCVg8i0hZJc9gxenOYSGWhu8EKySCiTYxpTxRuzFW3WkgBcl3QKCjGdFWxMBC4xkpMDYBgGLc+fFclObI/VqKkKARlKFAto2I1CyBM/NhmCajWF/f3h8fy6J+CGE7CY8HzM74373vZvPl5mUZz/73cURDofDAgAAwAVdFe0FAAAADAREEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADMRHewGDydmzZ3XixAklJSXJ4XBEezkAAMBAOBzW6dOn5fF4dNVVn7+fRDT1oRMnTig9PT3aywAAAL1w7NgxXXPNNZ/7ONHUh5KSkiR9+oeenJwc5dUAAAATra2tSk9Pt/8e/zxEUx8695ZccnIy0QQAwADzRZfWcCE4AACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAbio70AAMCnsh95MdpLAGJS/cp7o70ESew0AQAAGCGaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADUY2m3/3ud7rjjjvk8XjkcDj06quvRjweDoe1bNkyeTweJSYmKi8vTwcPHoyY6ejo0Pz585Wamqrhw4erqKhIx48fj5gJBALyer2yLEuWZcnr9aqlpSVi5ujRo7rjjjs0fPhwpaamqrS0VKFQ6HKcNgAAGICiGk1nzpzRDTfcoIqKih4fX7FihVatWqWKigrt27dPbrdb06dP1+nTp+0Zn8+nqqoqbdu2TTt37lRbW5sKCwvV1dVlzxQXF6uhoUHV1dWqrq5WQ0ODvF6v/XhXV5e+/e1v68yZM9q5c6e2bdumV155RQsXLrx8Jw8AAAYURzgcDkd7EZLkcDhUVVWlmTNnSvp0l8nj8cjn82nJkiWSPt1VcrlcWr58uebNm6dgMKhRo0Zp8+bNuvvuuyVJJ06cUHp6ul577TUVFBTo0KFDGj9+vOrq6pSTkyNJqqurU25urg4fPqyxY8fq17/+tQoLC3Xs2DF5PB5J0rZt23T//ferublZycnJRufQ2toqy7IUDAaNnwMA52Q/8mK0lwDEpPqV917W1zf9+ztmr2lqbGyU3+9Xfn6+fczpdGrKlCnatWuXJKm+vl6dnZ0RMx6PR1lZWfbM7t27ZVmWHUySNGnSJFmWFTGTlZVlB5MkFRQUqKOjQ/X19Z+7xo6ODrW2tkbcAADA4BSz0eT3+yVJLpcr4rjL5bIf8/v9SkhI0IgRIy44k5aW1u3109LSImbO/zkjRoxQQkKCPdOT8vJy+zopy7KUnp5+kWcJAAAGipiNpnMcDkfE/XA43O3Y+c6f6Wm+NzPnW7p0qYLBoH07duzYBdcFAAAGrpiNJrfbLUnddnqam5vtXSG3261QKKRAIHDBmZMnT3Z7/VOnTkXMnP9zAoGAOjs7u+1AfZbT6VRycnLEDQAADE4xG00ZGRlyu92qqamxj4VCIdXW1mry5MmSpOzsbA0ZMiRipqmpSQcOHLBncnNzFQwGtXfvXntmz549CgaDETMHDhxQU1OTPbNjxw45nU5lZ2df1vMEAAADQ3w0f3hbW5v+9Kc/2fcbGxvV0NCglJQUjRkzRj6fT2VlZcrMzFRmZqbKyso0bNgwFRcXS5Isy9Ls2bO1cOFCjRw5UikpKVq0aJEmTJigadOmSZLGjRunGTNmaM6cOVq/fr0kae7cuSosLNTYsWMlSfn5+Ro/fry8Xq9Wrlypjz76SIsWLdKcOXPYPQIAAJKiHE2///3v9c1vftO+v2DBAknSfffdp8rKSi1evFjt7e0qKSlRIBBQTk6OduzYoaSkJPs5q1evVnx8vGbNmqX29nZNnTpVlZWViouLs2e2bt2q0tJS+1N2RUVFEd8NFRcXp1/96lcqKSnRzTffrMTERBUXF+vZZ5+93H8EAABggIiZ72kaDPieJgCXgu9pAnrG9zQBAAAMIEQTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGYjqaPvnkE/3bv/2bMjIylJiYqOuuu05PPfWUzp49a8+Ew2EtW7ZMHo9HiYmJysvL08GDByNep6OjQ/Pnz1dqaqqGDx+uoqIiHT9+PGImEAjI6/XKsixZliWv16uWlpb+OE0AADAAxHQ0LV++XD/96U9VUVGhQ4cOacWKFVq5cqXWrl1rz6xYsUKrVq1SRUWF9u3bJ7fbrenTp+v06dP2jM/nU1VVlbZt26adO3eqra1NhYWF6urqsmeKi4vV0NCg6upqVVdXq6GhQV6vt1/PFwAAxC5HOBwOR3sRn6ewsFAul0svvPCCfeyf/umfNGzYMG3evFnhcFgej0c+n09LliyR9Omuksvl0vLlyzVv3jwFg0GNGjVKmzdv1t133y1JOnHihNLT0/Xaa6+poKBAhw4d0vjx41VXV6ecnBxJUl1dnXJzc3X48GGNHTvWaL2tra2yLEvBYFDJycl9/KcBYLDLfuTFaC8BiEn1K++9rK9v+vd3TO80feMb39Drr7+u9957T5L0hz/8QTt37tS3vvUtSVJjY6P8fr/y8/Pt5zidTk2ZMkW7du2SJNXX16uzszNixuPxKCsry57ZvXu3LMuyg0mSJk2aJMuy7JmedHR0qLW1NeIGAAAGp/hoL+BClixZomAwqOuvv15xcXHq6urS008/rX/+53+WJPn9fkmSy+WKeJ7L5dIHH3xgzyQkJGjEiBHdZs493+/3Ky0trdvPT0tLs2d6Ul5erieffLL3JwgAAAaMmN5pevnll7Vlyxa99NJLevvtt7Vp0yY9++yz2rRpU8Scw+GIuB8Oh7sdO9/5Mz3Nf9HrLF26VMFg0L4dO3bM5LQAAMAAFNM7TY888ogeffRRffe735UkTZgwQR988IHKy8t13333ye12S/p0p2j06NH285qbm+3dJ7fbrVAopEAgELHb1NzcrMmTJ9szJ0+e7PbzT5061W0X67OcTqecTuelnygAAIh5Mb3T9PHHH+uqqyKXGBcXZ3/lQEZGhtxut2pqauzHQ6GQamtr7SDKzs7WkCFDImaampp04MABeyY3N1fBYFB79+61Z/bs2aNgMGjPAACAK1tM7zTdcccdevrppzVmzBh95Stf0f79+7Vq1So98MADkj59S83n86msrEyZmZnKzMxUWVmZhg0bpuLiYkmSZVmaPXu2Fi5cqJEjRyolJUWLFi3ShAkTNG3aNEnSuHHjNGPGDM2ZM0fr16+XJM2dO1eFhYXGn5wDAACDW0xH09q1a/X444+rpKREzc3N8ng8mjdvnn74wx/aM4sXL1Z7e7tKSkoUCASUk5OjHTt2KCkpyZ5ZvXq14uPjNWvWLLW3t2vq1KmqrKxUXFycPbN161aVlpban7IrKipSRUVF/50sAACIaTH9PU0DDd/TBOBS8D1NQM/4niYAAIABhGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMBAzEfTX//6V/3Lv/yLRo4cqWHDhulrX/ua6uvr7cfD4bCWLVsmj8ejxMRE5eXl6eDBgxGv0dHRofnz5ys1NVXDhw9XUVGRjh8/HjETCATk9XplWZYsy5LX61VLS0t/nCIAABgAYjqaAoGAbr75Zg0ZMkS//vWv9cc//lE/+clP9KUvfcmeWbFihVatWqWKigrt27dPbrdb06dP1+nTp+0Zn8+nqqoqbdu2TTt37lRbW5sKCwvV1dVlzxQXF6uhoUHV1dWqrq5WQ0ODvF5vf54uAACIYY5wOByO9iI+z6OPPqq33npLb775Zo+Ph8NheTwe+Xw+LVmyRNKnu0oul0vLly/XvHnzFAwGNWrUKG3evFl33323JOnEiRNKT0/Xa6+9poKCAh06dEjjx49XXV2dcnJyJEl1dXXKzc3V4cOHNXbsWKP1tra2yrIsBYNBJScn98GfAIArSfYjL0Z7CUBMql9572V9fdO/v3u103Tbbbf1+NZVa2urbrvttt68ZI+2b9+uiRMn6q677lJaWppuvPFGbdy40X68sbFRfr9f+fn59jGn06kpU6Zo165dkqT6+np1dnZGzHg8HmVlZdkzu3fvlmVZdjBJ0qRJk2RZlj3Tk46ODrW2tkbcAADA4NSraHrjjTcUCoW6Hf/b3/72ubtCvfGXv/xF69atU2Zmpv77v/9bDz74oEpLS/Xii5/+vzG/3y9JcrlcEc9zuVz2Y36/XwkJCRoxYsQFZ9LS0rr9/LS0NHumJ+Xl5fY1UJZlKT09vfcnCwAAYlr8xQy/88479n//8Y9/jAiKrq4uVVdX6+/+7u/6bHFnz57VxIkTVVZWJkm68cYbdfDgQa1bt0733vv/t+ocDkfE88LhcLdj5zt/pqf5L3qdpUuXasGCBfb91tZWwgkAgEHqoqLpa1/7mhwOhxwOR49vwyUmJmrt2rV9trjRo0dr/PjxEcfGjRunV155RZLkdrslfbpTNHr0aHumubnZ3n1yu90KhUIKBAIRu03Nzc2aPHmyPXPy5MluP//UqVPddrE+y+l0yul09vLsAADAQHJRb881Njbqz3/+s8LhsPbu3avGxkb79te//lWtra164IEH+mxxN998s44cORJx7L333tO1114rScrIyJDb7VZNTY39eCgUUm1trR1E2dnZGjJkSMRMU1OTDhw4YM/k5uYqGAxq79699syePXsUDAbtGQAAcGW7qJ2mc7Fy9uzZy7KY8/3gBz/Q5MmTVVZWplmzZmnv3r3asGGDNmzYIOnTt9R8Pp/KysqUmZmpzMxMlZWVadiwYSouLpYkWZal2bNna+HChRo5cqRSUlK0aNEiTZgwQdOmTZP06e7VjBkzNGfOHK1fv16SNHfuXBUWFhp/cg4AAAxuFxVNn/Xee+/pjTfeUHNzc7eI+uEPf3jJC5Okm266SVVVVVq6dKmeeuopZWRkaM2aNbrnnnvsmcWLF6u9vV0lJSUKBALKycnRjh07lJSUZM+sXr1a8fHxmjVrltrb2zV16lRVVlYqLi7Ontm6datKS0vtT9kVFRWpoqKiT84DAAAMfL36nqaNGzfqX//1X5Wamiq3293tguq33367Txc5UPA9TQAuBd/TBPQsVr6nqVc7TT/+8Y/19NNP218oCQAAMNj16nuaAoGA7rrrrr5eCwAAQMzqVTTddddd2rFjR1+vBQAAIGb16u25L3/5y3r88cdVV1enCRMmaMiQIRGPl5aW9sniAAAAYkWvomnDhg26+uqrVVtbq9ra2ojHHA4H0QQAAAadXkVTY2NjX68DAAAgpvXqmiYAAIArTa92mr7on0r52c9+1qvFAAAAxKpeRVMgEIi439nZqQMHDqilpaXHf8gXAABgoOtVNFVVVXU7dvbsWZWUlOi666675EUBAADEmj67pumqq67SD37wA61evbqvXhIAACBm9OmF4H/+85/1ySef9OVLAgAAxIRevT23YMGCiPvhcFhNTU361a9+pfvuu69PFgYAABBLehVN+/fvj7h/1VVXadSoUfrJT37yhZ+sAwAAGIh6FU2//e1v+3odAAAAMa1X0XTOqVOndOTIETkcDv3DP/yDRo0a1VfrAgAAiCm9uhD8zJkzeuCBBzR69GjdeuutuuWWW+TxeDR79mx9/PHHfb1GAACAqOtVNC1YsEC1tbX6r//6L7W0tKilpUW//OUvVVtbq4ULF/b1GgEAAKKuV2/PvfLKK/qP//gP5eXl2ce+9a1vKTExUbNmzdK6dev6an0AAAAxoVc7TR9//LFcLle342lpabw9BwAABqVeRVNubq6eeOIJ/e1vf7OPtbe368knn1Rubm6fLQ4AACBW9OrtuTVr1uj222/XNddcoxtuuEEOh0MNDQ1yOp3asWNHX68RAAAg6noVTRMmTND777+vLVu26PDhwwqHw/rud7+re+65R4mJiX29RgAAgKjrVTSVl5fL5XJpzpw5Ecd/9rOf6dSpU1qyZEmfLA4AACBW9OqapvXr1+v666/vdvwrX/mKfvrTn17yogAAAGJNr6LJ7/dr9OjR3Y6PGjVKTU1Nl7woAACAWNOraEpPT9dbb73V7fhbb70lj8dzyYsCAACINb26pul73/uefD6fOjs7ddttt0mSXn/9dS1evJhvBAcAAINSr6Jp8eLF+uijj1RSUqJQKCRJGjp0qJYsWaKlS5f26QIBAABiQa+iyeFwaPny5Xr88cd16NAhJSYmKjMzU06ns6/XBwAAEBN6FU3nXH311brpppv6ai0AAAAxq1cXggMAAFxpiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGBlQ0lZeXy+FwyOfz2cfC4bCWLVsmj8ejxMRE5eXl6eDBgxHP6+jo0Pz585Wamqrhw4erqKhIx48fj5gJBALyer2yLEuWZcnr9aqlpaUfzgoAAAwEAyaa9u3bpw0bNuirX/1qxPEVK1Zo1apVqqio0L59++R2uzV9+nSdPn3anvH5fKqqqtK2bdu0c+dOtbW1qbCwUF1dXfZMcXGxGhoaVF1drerqajU0NMjr9fbb+QEAgNgWH+0FmGhra9M999yjjRs36sc//rF9PBwOa82aNXrsscd05513SpI2bdokl8ull156SfPmzVMwGNQLL7ygzZs3a9q0aZKkLVu2KD09Xb/5zW9UUFCgQ4cOqbq6WnV1dcrJyZEkbdy4Ubm5uTpy5IjGjh3b/yf9ObIfeTHaSwBiUv3Ke6O9BACD3IDYaXrooYf07W9/246ecxobG+X3+5Wfn28fczqdmjJlinbt2iVJqq+vV2dnZ8SMx+NRVlaWPbN7925ZlmUHkyRNmjRJlmXZMz3p6OhQa2trxA0AAAxOMb/TtG3bNr399tvat29ft8f8fr8kyeVyRRx3uVz64IMP7JmEhASNGDGi28y55/v9fqWlpXV7/bS0NHumJ+Xl5XryyScv7oQAAMCAFNM7TceOHdP3v/99bdmyRUOHDv3cOYfDEXE/HA53O3a+82d6mv+i11m6dKmCwaB9O3bs2AV/JgAAGLhiOprq6+vV3Nys7OxsxcfHKz4+XrW1tXruuecUHx9v7zCdvxvU3NxsP+Z2uxUKhRQIBC44c/LkyW4//9SpU912sT7L6XQqOTk54gYAAAanmI6mqVOn6t1331VDQ4N9mzhxou655x41NDTouuuuk9vtVk1Njf2cUCik2tpaTZ48WZKUnZ2tIUOGRMw0NTXpwIED9kxubq6CwaD27t1rz+zZs0fBYNCeAQAAV7aYvqYpKSlJWVlZEceGDx+ukSNH2sd9Pp/KysqUmZmpzMxMlZWVadiwYSouLpYkWZal2bNna+HChRo5cqRSUlK0aNEiTZgwwb6wfNy4cZoxY4bmzJmj9evXS5Lmzp2rwsLCmPrkHAAAiJ6YjiYTixcvVnt7u0pKShQIBJSTk6MdO3YoKSnJnlm9erXi4+M1a9Ystbe3a+rUqaqsrFRcXJw9s3XrVpWWltqfsisqKlJFRUW/nw8AAIhNjnA4HI72IgaL1tZWWZalYDB42a5v4nuagJ4Nhu9p4vcb6Nnl/v02/fs7pq9pAgAAiBVEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABmI6msrLy3XTTTcpKSlJaWlpmjlzpo4cORIxEw6HtWzZMnk8HiUmJiovL08HDx6MmOno6ND8+fOVmpqq4cOHq6ioSMePH4+YCQQC8nq9sixLlmXJ6/WqpaXlcp8iAAAYIGI6mmpra/XQQw+prq5ONTU1+uSTT5Sfn68zZ87YMytWrNCqVatUUVGhffv2ye12a/r06Tp9+rQ94/P5VFVVpW3btmnnzp1qa2tTYWGhurq67Jni4mI1NDSourpa1dXVamhokNfr7dfzBQAAsSs+2gu4kOrq6oj7P//5z5WWlqb6+nrdeuutCofDWrNmjR577DHdeeedkqRNmzbJ5XLppZde0rx58xQMBvXCCy9o8+bNmjZtmiRpy5YtSk9P129+8xsVFBTo0KFDqq6uVl1dnXJyciRJGzduVG5uro4cOaKxY8f274kDAICYE9M7TecLBoOSpJSUFElSY2Oj/H6/8vPz7Rmn06kpU6Zo165dkqT6+np1dnZGzHg8HmVlZdkzu3fvlmVZdjBJ0qRJk2RZlj3Tk46ODrW2tkbcAADA4DRgoikcDmvBggX6xje+oaysLEmS3++XJLlcrohZl8tlP+b3+5WQkKARI0ZccCYtLa3bz0xLS7NnelJeXm5fA2VZltLT03t/ggAAIKYNmGh6+OGH9c477+gXv/hFt8ccDkfE/XA43O3Y+c6f6Wn+i15n6dKlCgaD9u3YsWNfdBoAAGCAGhDRNH/+fG3fvl2//e1vdc0119jH3W63JHXbDWpubrZ3n9xut0KhkAKBwAVnTp482e3nnjp1qtsu1mc5nU4lJydH3AAAwOAU09EUDof18MMP6z//8z/1P//zP8rIyIh4PCMjQ263WzU1NfaxUCik2tpaTZ48WZKUnZ2tIUOGRMw0NTXpwIED9kxubq6CwaD27t1rz+zZs0fBYNCeAQAAV7aY/vTcQw89pJdeekm//OUvlZSUZO8oWZalxMREORwO+Xw+lZWVKTMzU5mZmSorK9OwYcNUXFxsz86ePVsLFy7UyJEjlZKSokWLFmnChAn2p+nGjRunGTNmaM6cOVq/fr0kae7cuSosLOSTcwAAQFKMR9O6deskSXl5eRHHf/7zn+v++++XJC1evFjt7e0qKSlRIBBQTk6OduzYoaSkJHt+9erVio+P16xZs9Te3q6pU6eqsrJScXFx9szWrVtVWlpqf8quqKhIFRUVl/cEAQDAgOEIh8PhaC9isGhtbZVlWQoGg5ft+qbsR168LK8LDHT1K++N9hIuGb/fQM8u9++36d/fMX1NEwAAQKwgmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0nef5559XRkaGhg4dquzsbL355pvRXhIAAIgBRNNnvPzyy/L5fHrssce0f/9+3XLLLbr99tt19OjRaC8NAABEGdH0GatWrdLs2bP1ve99T+PGjdOaNWuUnp6udevWRXtpAAAgyuKjvYBYEQqFVF9fr0cffTTieH5+vnbt2tXjczo6OtTR0WHfDwaDkqTW1tbLts6ujvbL9trAQHY5f+/6C7/fQM8u9+/3udcPh8MXnCOa/s+HH36orq4uuVyuiOMul0t+v7/H55SXl+vJJ5/sdjw9Pf2yrBHA57PWPhjtJQC4TPrr9/v06dOyLOtzHyeazuNwOCLuh8PhbsfOWbp0qRYsWGDfP3v2rD766CONHDnyc5+DwaO1tVXp6ek6duyYkpOTo70cAH2I3+8rSzgc1unTp+XxeC44RzT9n9TUVMXFxXXbVWpubu62+3SO0+mU0+mMOPalL33pci0RMSo5OZn/UQUGKX6/rxwX2mE6hwvB/09CQoKys7NVU1MTcbympkaTJ0+O0qoAAECsYKfpMxYsWCCv16uJEycqNzdXGzZs0NGjR/Xgg1wrAQDAlY5o+oy7775b//u//6unnnpKTU1NysrK0muvvaZrr7022ktDDHI6nXriiSe6vUULYODj9xs9cYS/6PN1AAAA4JomAAAAE0QTAACAAaIJAADAANEEAABggGgCeuH5559XRkaGhg4dquzsbL355pvRXhKAPvC73/1Od9xxhzwejxwOh1599dVoLwkxhGgCLtLLL78sn8+nxx57TPv379ctt9yi22+/XUePHo320gBcojNnzuiGG25QRUVFtJeCGMRXDgAXKScnR1//+te1bt06+9i4ceM0c+ZMlZeXR3FlAPqSw+FQVVWVZs6cGe2lIEaw0wRchFAopPr6euXn50ccz8/P165du6K0KgBAfyCagIvw4Ycfqqurq9s/4uxyubr9Y88AgMGFaAJ6weFwRNwPh8PdjgEABheiCbgIqampiouL67ar1Nzc3G33CQAwuBBNwEVISEhQdna2ampqIo7X1NRo8uTJUVoVAKA/xEd7AcBAs2DBAnm9Xk2cOFG5ubnasGGDjh49qgcffDDaSwNwidra2vSnP/3Jvt/Y2KiGhgalpKRozJgxUVwZYgFfOQD0wvPPP68VK1aoqalJWVlZWr16tW699dZoLwvAJXrjjTf0zW9+s9vx++67T5WVlf2/IMQUogkAAMAA1zQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAG4YuTl5cnn8xnNvvHGG3I4HGppabmkn/n3f//3WrNmzSW9BoDYQDQBAAAYIJoAAAAMEE0ArkhbtmzRxIkTlZSUJLfbreLiYjU3N3ebe+utt3TDDTdo6NChysnJ0bvvvhvx+K5du3TrrbcqMTFR6enpKi0t1ZkzZ/rrNAD0I6IJwBUpFArpRz/6kf7whz/o1VdfVWNjo+6///5uc4888oieffZZ7du3T2lpaSoqKlJnZ6ck6d1331VBQYHuvPNOvfPOO3r55Ze1c+dOPfzww/18NgD6Q3y0FwAA0fDAAw/Y/33dddfpueee0z/+4z+qra1NV199tf3YE088oenTp0uSNm3apGuuuUZVVVWaNWuWVq5cqeLiYvvi8szMTD333HOaMmWK1q1bp6FDh/brOQG4vNhpAnBF2r9/v77zne/o2muvVVJSkvLy8iRJR48ejZjLzc21/zslJUVjx47VoUOHJEn19fWqrKzU1Vdfbd8KCgp09uxZNTY29tu5AOgf7DQBuOKcOXNG+fn5ys/P15YtWzRq1CgdPXpUBQUFCoVCX/h8h8MhSTp79qzmzZun0tLSbjNjxozp83UDiC6iCcAV5/Dhw/rwww/1zDPPKD09XZL0+9//vsfZuro6O4ACgYDee+89XX/99ZKkr3/96zp48KC+/OUv98/CAUQVb88BuOKMGTNGCQkJWrt2rf7yl79o+/bt+tGPftTj7FNPPaXXX39dBw4c0P3336/U1FTNnDlTkrRkyRLt3r1bDz30kBoaGvT+++9r+/btmj9/fj+eDYD+QjQBuOKMGjVKlZWV+vd//3eNHz9ezzzzjJ599tkeZ5955hl9//vfV3Z2tpqamrR9+3YlJCRIkr761a+qtrZW77//vm655RbdeOONevzxxzV69Oj+PB0A/cQRDofD0V4EAABArGOnCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAw8P8AsFrLWA4ATWsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb78767",
   "metadata": {},
   "source": [
    "### Let's split data \n",
    "\n",
    "Since our data is imbalanced with only about 4000 normal points and 10000 abnormal points, we will use stratified splitting.\n",
    "\n",
    "A stratified split means you divide the data so that each group (or class) keeps the same balance in both the training and testing sets as it had in the full dataset.\n",
    "\n",
    "Also doing a 70/15/15 split of Train, Test and Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90a5b57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of abnormal samples in original dataset: label\n",
      "1    0.721963\n",
      "0    0.278037\n",
      "Name: proportion, dtype: float64\n",
      "Percentage of abnormal samples in training set: label\n",
      "1    0.721963\n",
      "0    0.278037\n",
      "Name: proportion, dtype: float64\n",
      "Percentage of abnormal samples in test set: label\n",
      "1    0.721942\n",
      "0    0.278058\n",
      "Name: proportion, dtype: float64\n",
      "Total number of samples in training set: 10513\n",
      "Total number of samples in test set: 2183\n",
      "Total number of samples in validation set: 1856\n",
      "Total number of samples in y_train dataset: 10513\n",
      "Total number of samples in y_test dataset: 2183\n",
      "Total number of samples in y_val dataset: 1856\n",
      "Class distribution in training set: label\n",
      "1    0.721963\n",
      "0    0.278037\n",
      "Name: proportion, dtype: float64\n",
      "Class distribution in test set: label\n",
      "1    0.721942\n",
      "0    0.278058\n",
      "Name: proportion, dtype: float64\n",
      "Class distribution in validation set: label\n",
      "1    0.721983\n",
      "0    0.278017\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.15, random_state=42, stratify=y_temp)\n",
    "\n",
    "'''\n",
    "To check if stratification worked correctly, we will calculate:\n",
    "\n",
    "The percentage of abnormal samples in your original dataset\n",
    "The percentage of abnormal samples in your training set\n",
    "The percentage of abnormal samples in your test set\n",
    "'''\n",
    "\n",
    "print(f\"Percentage of abnormal samples in original dataset: {y.value_counts(normalize=True)}\")\n",
    "print(f\"Percentage of abnormal samples in training set: {y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Percentage of abnormal samples in test set: {y_test.value_counts(normalize=True)}\")\n",
    "\n",
    "\n",
    "# Total number of samples in each set\n",
    "print(f\"Total number of samples in training set: {len(X_train)}\")\n",
    "print(f\"Total number of samples in test set: {len(X_test)}\")\n",
    "print(f\"Total number of samples in validation set: {len(X_val)}\")\n",
    "print(f\"Total number of samples in y_train dataset: {len(y_train)}\")\n",
    "print(f\"Total number of samples in y_test dataset: {len(y_test)}\")\n",
    "print(f\"Total number of samples in y_val dataset: {len(y_val)}\")\n",
    "\n",
    "# Class distribution in each set\n",
    "print(f\"Class distribution in training set: {y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Class distribution in test set: {y_test.value_counts(normalize=True)}\")\n",
    "print(f\"Class distribution in validation set: {y_val.value_counts(normalize=True)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db4904",
   "metadata": {},
   "source": [
    "### It works as we can see ratio is very similar of about 72 to 28 in every subset\n",
    "\n",
    "## Now let's train the model by balancing and giving more weight to the minority class which is the normal one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a121d82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [1.79832364 0.69255599]\n",
      "Class weights dictionary: {0: 1.798323640095792, 1: 0.6925559947299078}\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Convert class weights to a dictionary\n",
    "class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(f\"Class weights dictionary: {class_weights_dict}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec96e65",
   "metadata": {},
   "source": [
    "What the 'balanced' class weighting does behind the scenes:\n",
    "\n",
    "It automatically calculates weights inversely proportional to class frequencies\n",
    "It gives higher importance to underrepresented classes (in our case, normal ECGs)\n",
    "It makes the model \"pay more attention\" to errors on minority classes during training\n",
    "It helps prevent the model from simply predicting the majority class all the time\n",
    "\n",
    "Inversely proportional in simple words means \"goes in the opposite direction\" - when one value goes up, the other goes down. For our class weights, it means the less frequent a class is in our data, the higher weight we give it.\n",
    "The ratio between our weights (~2.6) corresponds well with our class imbalance ratio (~2.6, since we have about 72% abnormal and 28% normal). This suggests the weighting is appropriately calibrated to our dataset's specific imbalance.\n",
    "The true effectiveness of this weighting approach will only be fully validated once we train our model and evaluate its performance across metrics like accuracy, precision, recall, and F1 score. The weights look theoretically appropriate, but their practical impact depends on how our CNN model interacts with the specific patterns in our ECG data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b41f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_reshaped shape: (10513, 187, 1)\n",
      "X_val_reshaped shape: (1856, 187, 1)\n",
      "X_test_reshaped shape: (2183, 187, 1)\n",
      "Epoch 1/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.5341 - accuracy: 0.7301 - val_loss: 0.4729 - val_accuracy: 0.7694\n",
      "Epoch 2/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.4356 - accuracy: 0.7878 - val_loss: 0.4299 - val_accuracy: 0.8006\n",
      "Epoch 3/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.4143 - accuracy: 0.7998 - val_loss: 0.3863 - val_accuracy: 0.8265\n",
      "Epoch 4/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3967 - accuracy: 0.8123 - val_loss: 0.4030 - val_accuracy: 0.8125\n",
      "Epoch 5/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3854 - accuracy: 0.8191 - val_loss: 0.4213 - val_accuracy: 0.7963\n",
      "Epoch 6/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3824 - accuracy: 0.8191 - val_loss: 0.3576 - val_accuracy: 0.8416\n",
      "Epoch 7/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3721 - accuracy: 0.8223 - val_loss: 0.3484 - val_accuracy: 0.8459\n",
      "Epoch 8/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3659 - accuracy: 0.8300 - val_loss: 0.3665 - val_accuracy: 0.8362\n",
      "Epoch 9/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3610 - accuracy: 0.8314 - val_loss: 0.4676 - val_accuracy: 0.7716\n",
      "Epoch 10/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3566 - accuracy: 0.8346 - val_loss: 0.3603 - val_accuracy: 0.8411\n",
      "Epoch 11/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3499 - accuracy: 0.8406 - val_loss: 0.3731 - val_accuracy: 0.8319\n",
      "Epoch 12/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3484 - accuracy: 0.8402 - val_loss: 0.3697 - val_accuracy: 0.8357\n",
      "Epoch 13/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3442 - accuracy: 0.8421 - val_loss: 0.3714 - val_accuracy: 0.8362\n",
      "Epoch 14/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 0.3407 - accuracy: 0.8435 - val_loss: 0.4329 - val_accuracy: 0.7947\n",
      "Epoch 15/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 0.3390 - accuracy: 0.8450 - val_loss: 0.4827 - val_accuracy: 0.7667\n",
      "Epoch 16/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3336 - accuracy: 0.8482 - val_loss: 0.3395 - val_accuracy: 0.8561\n",
      "Epoch 17/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3319 - accuracy: 0.8494 - val_loss: 0.3252 - val_accuracy: 0.8702\n",
      "Epoch 18/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 0.3312 - accuracy: 0.8507 - val_loss: 0.3931 - val_accuracy: 0.8238\n",
      "Epoch 19/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3281 - accuracy: 0.8537 - val_loss: 0.3452 - val_accuracy: 0.8534\n",
      "Epoch 20/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3275 - accuracy: 0.8501 - val_loss: 0.3369 - val_accuracy: 0.8567\n",
      "Epoch 21/30\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 0.3288 - accuracy: 0.8513 - val_loss: 0.3584 - val_accuracy: 0.8454\n",
      "Epoch 22/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3238 - accuracy: 0.8532 - val_loss: 0.3055 - val_accuracy: 0.8815\n",
      "Epoch 23/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3224 - accuracy: 0.8531 - val_loss: 0.3985 - val_accuracy: 0.8168\n",
      "Epoch 24/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3184 - accuracy: 0.8564 - val_loss: 0.3203 - val_accuracy: 0.8685\n",
      "Epoch 25/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3183 - accuracy: 0.8569 - val_loss: 0.3737 - val_accuracy: 0.8367\n",
      "Epoch 26/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3156 - accuracy: 0.8576 - val_loss: 0.3076 - val_accuracy: 0.8723\n",
      "Epoch 27/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3138 - accuracy: 0.8604 - val_loss: 0.4011 - val_accuracy: 0.8211\n",
      "Epoch 28/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3120 - accuracy: 0.8615 - val_loss: 0.4339 - val_accuracy: 0.8012\n",
      "Epoch 29/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3115 - accuracy: 0.8615 - val_loss: 0.3238 - val_accuracy: 0.8653\n",
      "Epoch 30/30\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.3071 - accuracy: 0.8641 - val_loss: 0.3110 - val_accuracy: 0.8718\n"
     ]
    }
   ],
   "source": [
    "# Reshape the data to be 3D (samples, timesteps, features) Since we have only one feature (the ECG signal), the number of features is 1.\n",
    "X_train_array = X_train.values  \n",
    "X_val_array = X_val.values \n",
    "X_test_array = X_test.values  \n",
    "\n",
    "X_train_reshaped = X_train_array.reshape(X_train_array.shape[0], X_train_array.shape[1], 1)\n",
    "X_val_reshaped = X_val_array.reshape(X_val_array.shape[0], X_val_array.shape[1], 1)\n",
    "X_test_reshaped = X_test_array.reshape(X_test_array.shape[0], X_test_array.shape[1], 1)\n",
    "print(f\"X_train_reshaped shape: {X_train_reshaped.shape}\")\n",
    "print(f\"X_val_reshaped shape: {X_val_reshaped.shape}\")\n",
    "print(f\"X_test_reshaped shape: {X_test_reshaped.shape}\")\n",
    "\n",
    "model = Sequential()\n",
    "# Conv1D layer\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)))\n",
    "# MaxPooling1D layer\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# Flatten layer\n",
    "model.add(Flatten())\n",
    "# Dense layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_reshaped, y_train, \n",
    "                    epochs=30, \n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_val_reshaped, y_val),\n",
    "                    class_weight=class_weights_dict)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "486ffe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for F1 score calculation\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_pred_binary = K.cast(K.greater(y_pred, 0.5), K.floatx())\n",
    "    \n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred_binary, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred_binary, 0, 1)))\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def evaluate_model(X_train_reshaped, y_train, X_test_reshaped, y_test, class_weights_dict):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Conv1D layer with BatchNormalization\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, input_shape=(X_train_reshaped.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    \n",
    "    # First MaxPooling1D layer\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))  # First dropout layer\n",
    "    \n",
    "    # Second Conv1D layer with BatchNormalization\n",
    "    model.add(Conv1D(filters=64, kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    \n",
    "    # Second MaxPooling1D layer\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))  # Second dropout layer\n",
    "    \n",
    "    # Flatten layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Added a dense layer before the output\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))  # Third dropout layer with higher rate\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', \n",
    "                 loss='binary_crossentropy', \n",
    "                 metrics=['accuracy', Precision(), Recall(), AUC(), f1_score])\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_reshaped, y_train, \n",
    "              epochs=30, \n",
    "              batch_size=32,\n",
    "              class_weight=class_weights_dict,\n",
    "              verbose=0)\n",
    "    \n",
    "    # Evaluate model\n",
    "    results = model.evaluate(X_test_reshaped, y_test, verbose=0)\n",
    "\n",
    "    metrics = {\n",
    "        'loss': results[0],\n",
    "        'accuracy': results[1],\n",
    "        'precision': results[2],\n",
    "        'recall': results[3],\n",
    "        'auc': results[4],\n",
    "        'f1_score': results[5]\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dfb915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 1/1:\n",
      "loss: 11.66%\n",
      "accuracy: 95.92%\n",
      "precision: 97.88%\n",
      "recall: 96.45%\n",
      "auc: 98.92%\n",
      "f1_score: 97.15%\n",
      "Loss: 0.117% (+/-0.000)\n",
      "Accuracy: 0.959% (+/-0.000)\n",
      "Precision: 0.979% (+/-0.000)\n",
      "Recall: 0.964% (+/-0.000)\n",
      "Auc: 0.989% (+/-0.000)\n",
      "F1_score: 0.972% (+/-0.000)\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(X_train, y_train, X_test, y_test, class_weights_dict, repeats=1):\n",
    "    # Reshape data\n",
    "    X_train_array = X_train.values  \n",
    "    X_test_array = X_test.values  \n",
    "    \n",
    "    X_train_reshaped = X_train_array.reshape(X_train_array.shape[0], X_train_array.shape[1], 1)\n",
    "    X_test_reshaped = X_test_array.reshape(X_test_array.shape[0], X_test_array.shape[1], 1)\n",
    "    \n",
    "    all_metrics = {\n",
    "        'loss': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'auc': [],\n",
    "        'f1_score': []\n",
    "    }\n",
    "\n",
    "    for r in range(repeats):\n",
    "        metrics = evaluate_model(X_train_reshaped, y_train, X_test_reshaped, y_test, class_weights_dict)\n",
    "        \n",
    "        for key in all_metrics.keys():\n",
    "            all_metrics[key].append(metrics[key])\n",
    "\n",
    "        # Summary of metrics for this repeat\n",
    "        print(f\"Repeat {r+1}/{repeats}:\")\n",
    "        for key, value in all_metrics.items():\n",
    "            print(f\"{key}: {value * 100:.2f}%\")\n",
    "        \n",
    "        \n",
    "    # Summary of all repeats\n",
    "    for metric_name, values in all_metrics.items():\n",
    "        m, s = np.mean(values), np.std(values)\n",
    "        print(f'{metric_name.capitalize()}: {m:.3f}% (+/-{s:.3f})')\n",
    "\n",
    "    return all_metrics\n",
    "\n",
    "\n",
    "\n",
    "# Run the experiment with 10 repeats\n",
    "scores = run_experiment(X_train, y_train, X_test, y_test, class_weights_dict, repeats=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
